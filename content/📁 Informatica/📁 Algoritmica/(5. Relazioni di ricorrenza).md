---
draft: true
---
%% http://www.dacrema.com/Informatica/ricorr_linear_partizio.htm %%
La funzione tempo di un algoritmo ricorsivo è ricorsiva: come faccio quindi a determinarlo? Semplice: ci metto un'incognita, es $T(n-1)$.

Il problema è che non posso definire la funzione T usando se stessa:
$$
T(n) = \begin{cases}
c_1 + c_2 & n < 2 \\
T(n-1) + c_3 & n \ge 2
\end{cases}
$$
questa si chiama "relazione di ricorrenza" e va risolta.

Ci sono vari metodi per risolverla: ci sono 3 o 4 metodi, ma il metodo dell'unfolding è il più immediato: consideriamo $T(n) = T(n-1) + d$.
Riscriviamola per $k$ ricorsioni generiche:
$$
\begin{align}
T(n) & = T(n-1) + d \\
 & = T(n-2) + d + d = T(n-2) + 2d \\
 & = \ldots \\
 & = T(n-k) + kd
\end{align}
$$
Non possiamo sapere quanto vale $n$, ma quale che sia, non possiamo sottrargli all'infinito $k$ perché è limitato da $n$ stesso. Quindi questa relazione vale per ogni $k \le n$. Se ciò vale dappertutto e specialmente nell'estremo $k = n$, allora mi pongo proprio all'estremo: sostituisco $k$ con $n$, ottenendo:
$$
T(n) = T(n - n) + nd = T(0) + nd
$$
Sappiamo che $T(0) + nd$ è in $O(n)$ perché $T(0)$ è in $O(1)$ e $nd$ è in $O(n)$.

Esempi:
$$
\begin{align*} % Algorithm environment
 & \textbf{Min-Rec(A, i)} \\
 & \rhd \texttt{Pre: $ < m = length(A), 0 \le i < m$, quindi $A[i,\ldots,m-1] \ne \emptyset$} \\
 & \rhd \texttt{Post: ritorna il minimo in $A[i,\ldots,m-1]$} \\
 & \texttt{if } i = length(A) - 1 \texttt{ then} \quad \rhd A[m-1] \texttt{ è l'unico elemento} \\
 & \quad \texttt{return } A[i] \\
 & \texttt{else} \\
 & \quad \texttt{return } \min(A[i], \textbf{Min-Rec(A,i+1)}) \\
 & \texttt{end if} \\
 & \texttt{return}
\end{align*}
$$
La relazione di ricorrenza ha la forma $T(n) = T(n-1) + d$, dove $n$ è la dimensione dell'array $m-1$. Come abbiamo già visto, sappiamo che è lineare.
In realtà, tutti gli algoritmi di scansione di una struttura lineare (sia iterativi che ricorsivi) hanno questa forma.

Esempio con inserimento ordinato ricorsivo:
$$
\begin{align*} % Algorithm environment
 & \textbf{Insert-Ord-Rec(A,i)} \\
 & \rhd \texttt{Pre: } 0 \le i < length(A) \texttt{ e } A[0,\ldots,i-1] \texttt{ ordinato} \\
 & \rhd \texttt{Post: } A'[0,\ldots,i] \texttt{ permutazione ordinata di } A[0,\ldots,i] \\
 & \texttt{if } i \ge 1 \texttt{ and } A[i] < A[i-1] \texttt{ then} \\
 & \quad swap(A[i], A[i-1]) \\
 & \quad \textbf{Insert-Ord-Rec(A,i-1)} \\
 & \texttt{end if} \\
 & \texttt{return}
\end{align*}
$$
Abbiamo:
$$
T_{insOrd}(n) = T_{insOrd}(n-1) + d = O(n)
$$
dove $n = i - 1$ è l'ampiezza dell'intervallo in cui avviene l'inserimento.

$$
\begin{align*} % Algorithm environment
 & \textbf{Inser-Sort-Ric(A,i)} \\
 & \rhd \texttt{Pre: } m = length(A), 0 \le i \le m \texttt{ e } A[0,\ldots,i-1] \texttt{ordinato} \\
 & \rhd \texttt{Post: } A'[0,\ldots,i] \texttt{ permutazione ordinata di } A[0,\ldots,i] \\
 & \texttt{if } i = length(A) \texttt{ then} \quad \rhd i = m \texttt{ dunque } A[0,\ldots,m-1] \texttt{ è ordinato} \\
 & \quad \texttt{return } A \\
 & \texttt{else} \quad \rhd i < length(A) \\
 & \quad Insert-Ord-Rec(A,i) \quad \rhd A[0,\ldots,i] \texttt{ ordinato} \\
 & \quad \texttt{return } Insert-Sort-Ric(A, i + 1) \\
 & \texttt{end if} \\
 & \texttt{end}
\end{align*}
$$
Cosa faccio: se siamo nel passo ricorsivo, inserisco l'elemento a sinistra in modo ordinato e possiamo farlo perché così soddisfiamo sempre la pre-cond dell'algoritmo e poi facciamo la chiamata ricorsiva.
La relazione di ricorrenza è:
$$
T_{insS}(n) = \begin{cases}
c & n \le 1 \\
T_{insS}(n-1) + T_{insOrd}(n) & n > 1
\end{cases}
$$
cioè, è della forma $T(n) = T(n-1) + dn$ che è diversa da $T(n-1) + d$. Facciamo lo srotolamento:
$$
\begin{align}
T(n) & = T(n-1) + dn \\
 & = T(n-2) + d(n-1) + dn = T(n-2) + d(2n-1) \\
 & = \ldots \\
 & = T(n-k) + d \sum_{i=0}^{k-1}(n-i)
\end{align}
$$
dove $k \le n$.
Mi pongo nell'estremo $k = n$:
$$
\begin{align}
T(n) & = T(0) + d \sum_{i=0}^{n-1}(n-i) \\
 & = T(0) + d \sum_{i=1}^n i \\
 & = T(0) + d \frac{n(n+1)}{2}
\end{align}
$$
che è in $O(n^2)$ se $T(0) \in O(1)$.



Altro esempio: Hanoi
$$
\begin{align*} % Algorithm environment
 & \textbf{Hanoi(n, A, B, C)} \\
 & \rhd \texttt{Pre: la torre } A \texttt{ di } n \texttt{ dischi; diametro disco } n \texttt{-esimo di } A \texttt{ è minore del diametro del disco in cima a } B \\
 & \rhd \texttt{Post: sposta } A \texttt{ in } B \texttt{ usando } C \texttt{ come appoggio} \\
 & \texttt{if } n=1 \texttt{ then} \\
 & \quad \texttt{muovi un disco da } A \texttt{ a } B \\
 & \texttt{else} \\
 & \quad \textbf{Hanoi(n-1, A, C, B)} \\
 & \quad \texttt{muovi un disco da } A \texttt{ a } B \\
 & \quad \textbf{Hanoi(n-1, C, B, A)} \\
 & \texttt{end if} \\
 & \texttt{end}
\end{align*}
$$

Relazione di ricorrenza:
$$
T_{Hanoi}(n) = \begin{cases}
1 & n \le 1 \\
2T_{Hanoi}(n-1) + 1 & n > 1
\end{cases}
$$

Unfolding:
$$
\begin{align}
T(n) & = cT(n-1) + d \\
 & = c(cT(n-2) + d) + d = c^2T(n-2) + cd + d \\
 & = \ldots \\
 & = c^kT(n-k) + d\sum_{i=0}^{k-1} c^i
\end{align}
$$
dove $k \le n$. Poniamoci nell'estremo $k = n$:
$$
\begin{align}
T(n) & = c^nT(n-n) + d\sum_{i=0}^{n-1} c^i \\
 & = c^nT(0) + d\frac{c^n-1}{c-1}
\end{align}
$$
che è in $O(c^n)$ se $T(0) \in O(1)$ e $c \ne 1$, cioè è ESPONENZIALE.
$c$, nel caso nostro, è $2$%% perché? %% e diventa $T(n) = 2^nT(0) + d(2^n-1)$.

Anche se un po' diversi, tutti questi esempi sono casi particolari di questa forma generale di relazioni di ricorrenza:
$$
T(n) = \begin{cases}
d & n \le m \le h \\
\displaystyle\sum_{1 \le i \le h} (a_iT(n-i) )+ cn^\beta & n > m
\end{cases}
$$
e c'è una serie di teoremi che mostrano come si risolvono le relazioni di ricorrenza (a meno di termini costanti), tra cui questo qui (relazioni lineari a partizione costante) e il Master's Theorem (metodo dell'esperto o relazioni lineari a partizione bilanciata).
- La $h$ si chiama "ordine costante" e non può dipendere da $n$
- I coefficienti $a_i$ sono costanti
- $T$ si regredisce linearmente di un quantità costante $i$ e quindi vuol dire che c'è una partizione costante
- $i$ è limitato da $h$
- Lì dove c'è la chiamata ricorsiva c'è il lavoro polinomiale di divisione o combinazione $cn^\beta$ di grado $\beta$ (detta "lavoro" proprio perché il grado è sempre positivo o nullo).

Teorema delle relazioni lineari a partizione costante:
Se $c>0, \beta \ge 0, a = \displaystyle\sum_{1 \le i \le h} a_i$, allora
$$
\begin{cases}
T(n) \in O(n^{\beta + 1}) & a = 1 \\
T(n) \in O(a^nn^\beta) & a \ge 2
\end{cases}
$$

Insert-Sort ricorsivo:
$$
T(n) = n-1 + T(n-1) \le T(n-1) + cn
$$
per $c \ge 1$. Nella forma generica
$$
T(n) = \begin{cases}
d & n \le m \le h \\
\displaystyle\sum_{1 \le i \le h} (a_iT(n-i) )+ cn^\beta & n > m
\end{cases}
$$
abbiamo:
- $h = 1$
- $a = \displaystyle\sum_{1 \le i \le h} a_i = 1$
- $\beta = 1$ perché $n^1$

Dal teorema, risulta che $T(n) \in O(n^{\beta + 1}) = O(n^2)$ perché $a = 1$.


Altro esempio: Minimo ricorsivo (caso peggiore): $T(n) = T(n-1) + c$ per $c > 0$. Nella forma generica, abbiamo $h=1, a=1, \beta = 0$. Dato che $a=1$, abbiamo $T(n) \in O(n^{\beta + 1}) = O(n)$.



Altro esempio: Hanoi: $T(n) = 2T(n-1) + c$ per $c = 1$. Abbiamo $h = 1, a = 2, \beta = 0$, e dato che $a \ge 2$, allora $T(n) \in O(a^nn^\beta) = O(2^nn^0) = O(2^n)$.

# Divide et Impera

È un caso specifico di relazioni di ricorrenza lineari con partizione bilanciata.

"Per meglio dominare, occorre dividere gli avversari", ossia: suddividi il problema in sottoproblemi di dimensioni circa eguale; risolvi i sottoproblemi con la ricorsione, infine combina i risultati.
Più o meno, lo schema da seguire è questo:
$$
\begin{align*} % Algorithm environment
 & \textbf{DivideEtImpera(P,n)} \\
 & \rhd \texttt{Pre: } n \texttt{ è la dimensione di } P \\
 & \rhd \texttt{Post: } P' \texttt{ è il problema } P \texttt{ risolto}   \\
 & \texttt{if } n \le k \texttt{ then} \\
 & \quad \texttt{risolvi direttamente } P \\
 & \texttt{else} \\
 & \quad \texttt{dividi } P \texttt{ nei sottoproblemi } P_i, \ldots, P_h \texttt{ di dimensioni } n_1, \ldots, n_h \\
 & \quad \texttt{for } i \gets 1 \texttt{ to } h \texttt{ do} \\
 & \quad \quad P'_i \gets \textbf{DivideEtImpera}(P_i, n_i) \\
 & \quad \texttt{end for} \\
 & \quad \texttt{ricombina tutti i } P'_i \texttt{ in } P' \\ 
 & \texttt{end if} \\
 & \texttt{return } P'
\end{align*}
$$

Forma generale della relazione di ricorrenza del Divide et Impera:
$$
T(n) = \begin{cases}
d & n \le k \\
D(n) + C(n) + \displaystyle\sum_{i=1}^h T(n_i) & n > k
\end{cases}
$$
dove:
- $D(n)$ è il tempo per dividere il problema di grandezza $n$
- $C(n)$ è il tempo per combinare il problema di grandezza $n$
- $\displaystyle\sum_{i=1}^h T(n_i)$ è la somma dei tempi delle chiamate ricorsive degli $h$ sottoproblemi.

Prossimo metodo è efficace quando le $n_i$ sono in numero costante (cioè $h$ è fissa e una sola), questi $n_i$ devono essere circa bilanciati%% ??? %% e le chiamate ricorsive non devono essere sovrapposte, cioè non deve succedere che il lavoro fatto da una parte venga ripetuto dall'altra, altrimenti l'algoritmo non è efficiente e va utilizzata la programmazione dinamica.

## Esempio: l'algoritmo $DI-Min-Max$ calcola il min e max in $A[p,\ldots,q]$:
$$
\begin{align*} % Algorithm environment
 & \textbf{DI-Min-Max}(A,p,q) \\
 & \rhd \texttt{Pre: }  \\
 & \rhd \texttt{Post: }  \\
 & \texttt{if } p=q \texttt{ then} \\
 & \quad \texttt{return } (A[p],A[q]) \\
 & \texttt{else if } p = q - 1 \texttt{ then} \\
 & \quad \texttt{if } A[p] < A[q] \texttt{ then} \\
 & \quad \quad \texttt{return } (A[p], A[q]) \\
 & \quad \texttt{else} \\
 & \quad \quad \texttt{return } (A[q], A[p]) \\
 & r \gets \left\lfloor \frac{p+q}{2} \right\rfloor \\
 & (min_1, max_1) \gets \textbf{DI-Min-Max}(A,p,r) \\
 & (min_2, max_2) \gets \textbf{DI-Min-Max}(A,r+1,q) \\
 & \texttt{return } (\min(min_1,min_2), \max(max_1, max_2))
\end{align*}
$$
Simile alla tecnica dicotomica, divido a metà l'intervallo e procedo finché non arrivo ai singoletti.
Se $n = length(A[p,\ldots,q]) = q - p + 1$, allora i confronti $C(n)$ sono:
$$
C(n) = \begin{cases}
0 & n = 1 \\
1 & n = 2 \\
C\left( \left\lfloor \frac{n}{2} \right\rfloor \right) + C\left( \left\lceil \frac{n}{2} \right\rceil \right) + 2 & n > 2
\end{cases}
$$
dove $C\left( \left\lfloor \frac{n}{2} \right\rfloor \right)$ è la prima chiamata ricorsiva sulla prima metà, $C\left( \left\lceil \frac{n}{2} \right\rceil \right)$ sulla seconda e il $2$ sono le funzioni di $\min$ e $\max$ nel return.

Il metodo del guessing "la risposta è dentro di te, e, però, è sbagliata" (cit. Quelo, ripresa da De' Liguoro). Per semplicità, supponiamo $n = 2^k$ con $k > 1$:
$$
2 \left( \frac{3}{2} 2^k - 2 \right) + 2 = \frac{3}{2} 2^{k+1} - 4 + 2 = \frac{3}{2} 2^{k+1} - 2
$$
dove $2 \left( \frac{3}{2} 2^k - 2 \right) = 2C\left( \frac{n}{2} \right)$.

Quindi, ragionando per induzione su $k > 1$, se ne conclude che $C(2^k) = \frac{3}{2} 2^k - 2$, ossia che i confronti sono $\frac{3}{2}n - 2$.

## Esempio: binary search


$$
\begin{align*} % Algorithm environment
 & \textbf{BinSearch-Ric}(x, A, i, j) \\
 & \rhd \texttt{Pre: } A[i,\ldots,j] \texttt{ ordinato}  \\
 & \rhd \texttt{Post: } true \texttt{ se } x \in A[i,\ldots,j] \\
 & \texttt{if } i > j \texttt{ then} \quad \rhd A[i,\ldots,j] = \emptyset \\
 & \quad \texttt{return } false \\
 & \texttt{else} \\
 & \quad m \gets \left\lfloor \frac{i+j}{2} \right\rfloor \\
 & \quad \texttt{if } x = A[m] \texttt{ then} \\
 & \quad \quad \texttt{return } true \\
 & \quad \texttt{else if } x < A[m] \texttt{ then} \\
 & \quad \quad \texttt{return } \textbf{BinSearch-Ric}(x, A, i, m-1) \\
 & \quad \texttt{else} \quad \rhd x > A[m] \\
 & \quad \quad \texttt{return } \textbf{BinSearch-Ric}(x, A, m+1, j) \\
 & \texttt{end}
\end{align*}
$$
%% sistemare anche l'altro BinSearch nei file precedenti %%
La dimensione viene dimezzata a ogni chiamata ricorsiva e, inoltre, queste due chiamate sono mutualmente esclusive%% giusto dire così? %%, cioè viene eseguita o una o l'altra, quindi la relazione di ricorrenza si può scrivere così: $T(n) = T\left( \frac{n}{2} \right) + 1$ dove $1$ è la costante che rappresenta il lavoro effettuato nell'algoritmo oltre alla chiamata, come il controllo dell'insieme vuoto, il calcolo del mezzo, ecc. ed $n=j-i+1$ è l'ampiezza dello spazio di ricerca.

Applichiamo il metodo dell'unfolding%% sostituire 2 con b e 1 con d %%:
$$
\begin{align}
T(n) & = T\left( \frac{n}{2} \right) + 1 \\
 & = T\left( \frac{n}{2^2} \right) + 1 + 1 \\
 & = \ldots \\
 & = T\left( \frac{n}{2^k} \right) + k
\end{align}
$$
se $k \le \log_2n$. Mi pongo all'estremo $k = \log_2 n$:
$$
T(n) = T\left( \frac{n}{2^{\log_2n}} \right) + \log_2n = T\left( \frac{n}{n} \right) + \log_2n = T(1) + \log_2n
$$
che è in $O(\log n)$. Sapevamo già, infatti, che la ricerca dicotomica fosse logaritmica, e ora ne abbiamo la conferma.

## Merge-sort (ordinamento per fusione)

Collezione di valori che vengono divisi a metà, affidando a ogni metà a due amici diversi che si occuperanno di ordinarle. Ottenute le due metà ordinate, io dovrò combinarle insieme (fonderle) in modo ordinato.

$$
\begin{align*} % Algorithm environment
 & \textbf{Merge-Sort(A)} \\
 & \rhd \texttt{Pre: }  \\
 & \rhd \texttt{Post: }  \\
 & \texttt{if } length(A) = 1 \texttt{ then} \\
 & \quad \texttt{return } A \\
 & \texttt{else} \\
 & \quad k \gets \left\lfloor  \frac{length(A)}{2}  \right\rfloor \\
 & \quad B \gets \textbf{Merge-Sort}(A[1,\ldots,k]) \\
 & \quad C \gets \textbf{Merge-Sort}(A[k+1,\ldots,length(A)]) \\
 & \quad \texttt{return } \textbf{Merge}(B,C) \\
 & \texttt{end}
\end{align*}
$$

e

$$
\begin{align*} % Algorithm environment
 & \textbf{Merge}(B,C) \\
 & \rhd \textrm{Pre: }  \\
 & \rhd \textrm{Post: }  \\
 & \textrm{if } B = \emptyset \textrm{ then} \\
 & \quad \textrm{return } C \\
 & \textrm{else if } C = \emptyset \textrm{ then} \\
 & \quad \textrm{return } B \\
 & \textrm{else if } B[1] \le C[1] \textrm{ then} \\
 & \quad \textrm{return } [B[1], Merge(B[2,\ldots,length(B)], C)] \\
 & \textrm{else} \\
 & \quad \textrm{return } [C[1], Merge(B, C[2,\ldots,length(C)])] \\
\end{align*}
$$

Abbiamo $T_{Merge}(n) = T_{Merge}(n-1) + d$, cioè del primo tipo. Sappiamo già che fa $O(n)$.

### Esecuzione del Merge Sort

Abbiamo $T_{MergeSort}(n) = 2T\left( \left\lfloor \frac{n}{2} \right\rfloor \right) + T_{Merge}(n)$ e, sapendo che $T_{Merge}(n) \in O(n)$, possiamo sostituirlo per comodità con $n$:
$$
T(n) = \begin{cases}
c & n \le 1 \\
2T\left( \left\lfloor \frac{n}{2} \right\rfloor \right) + n & n > 1
\end{cases}
$$

Metodo dell'albero della ricorsione:
![](Pasted%20image%2020241229164830.png)

È nella forma:
$$
T(n) = \begin{cases}
d & n \le 1 \\
aT\left( \frac{n}{b} \right) + cn^\beta & n > 1
\end{cases}
$$
dove $a$ è il coefficiente costante, $T$ è una funzione lineare, le partizioni sono bilanciate perché $\frac{n}{b}$ e $cn^\beta$ è il lavoro polinomiale di divisione/combinazione.

## Teorema del maestro (o delle relazioni di ricorrenza lineari a partizione bilanciata)

Teorema: se $a \ge 1, b \ge 2, c > 0, d \ge 0, \beta \ge 0$, posto $\alpha = \frac{\log a}{\log b}$ (base del log non importa, basta che sia la stessa per entrambi), allora:
$$
\begin{cases}
T(n) \in O(n^\alpha) & \alpha > \beta \\
T(n) \in O(n^\alpha \log n) & \alpha = \beta \\
T(n) \in O(n^\beta) & \alpha < \beta
\end{cases}
$$
alfa esprime il bilancio tra la velocità con cui riduco la dimensione dell'ingresso e la quantità di lavoro (numero di chiamate) che devo fare per risolvere il problema.

Trucco: alfa domina, allora $O(n^\alpha)$; beta domina, allora $O(n^\beta)$; uguali, allora $O(n^\alpha \log n)$ o ugualmente $O(n^\beta \log n)$.

### Esempio: ricerca binaria

Abbiamo $T(n) = T\left( \frac{n}{2} \right) + c$, con $a = 1, b = 2, \alpha = \frac{\log 1}{\log 2} = 0 = \beta$, quindi $T(n) \in O(n^\alpha \log n) = O(n^0 \log n) = O(\log n)$.

### Esempio: MergeSort

Abbiamo $T(n) = 2T\left( \frac{n}{2} \right) + cn$ con $a=b=2, \alpha = \frac{\log 2}{\log 2} = 1 = \beta$, quindi
$$
\begin{align}
T(n) &  \in O(n^\alpha \log n) \\
 & = O(n^1 \log n) \\
 & = O(n \log n)
\end{align}
$$

Altri esempi: min-max-DI

## Quick-Sort (ordinamento per partizione)

È veloce solo nel caso medio. Inventato da Tony Hoare, nato nel 1934.
Idea: divido in due l'insieme scegliendo un elemento a caso (il pivot, in italiano "perno"), mettendo gli elementi minoranti a sinistra e i maggioranti a destra.

$$
\begin{align*} % Algorithm environment
 & \textbf{Quick-Sort(A)} \\
 & \rhd \textrm{Pre: }  \\
 & \rhd \textrm{Post: }  \\
 & \textrm{if } length(A) > 1 \textrm{ then} \\
 & \quad p \gets Partition(A) \\
 & \quad Quick-Sort(A[1,\ldots,p]) \\
 & \quad Quick-Sort(A[p+1,\ldots,length(A )])
\end{align*}
$$
e
$$
\begin{align*} % Algorithm environment
 & \textbf{Partition(A)} \\
 & \rhd \textrm{Pre: } length(A) = n > 1  \\
 & \rhd \textrm{Post: }  \\
 & pivot \gets A[1] \\
 & i \gets 2, j \gets length(A) \\
 & \textrm{while } i \le j \textrm{ do} \\
 & \quad \rhd \textrm{Inv: } \forall p \in A[2,\ldots,i-1], p \le pivot \land \forall q \in A[j+1,\ldots,n], q > pivot \\
 & \quad \textrm{if } A[i] \le pivot \textrm{ then} \\
 & \quad \quad i \gets i + 1 \\
 & \quad \textrm{else if } A[j] > pivot \textrm{ then} \\
 & \quad \quad j \gets j - 1 \\
 & \quad \textrm{else} \\
 & \quad \quad Swap(A[i],A[j]) \\
 & \quad \quad i \gets i + 1 \\
 & \quad \quad j \gets j - 1 \\
 & \rhd j = i - 1 \\
 & Swap(A[1],A[j]) \\
 & \textrm{return } j
\end{align*}
$$

![](Pasted%20image%2020241230034138.png)

Modifica l'array in modo DISTRUTTIVO, nel senso che non è che lo distrugge (non è un termine da intendere negativamente), ma che attua modifiche in-place e quindi modifica l'array in modo permanente senza possibilità di risalire all'ordine degli elementi iniziale.

Partition() è la funzione che associa a p il valore del pivot nell'array A.
Associando alla funzione tempo non il numero di operazioni, ma il numero di confronti, abbiamo che $T_{Partition}(n)=O(n)$ e $T_{QuickSort}(n)=T_{Partition}+T_{QuickSort}(p)+T_{QuickSort}(n-p)$ dove $p$ è la posizione del pivot. Il caso peggiore è quando $p=1$.
Posto $T_{Partition}=n-1$ e $T(1)=0$ (perché con un solo elemento non ci sono confronti):
$$
T(n)=n-1+T(n-p) \in \Theta(n^2)
$$
Significa che il QuickSort, nel caso peggiore, è di costo quadratico, esattamente come l'InsertSort, il SelectSort, il BubbleSort. Ciò che contraddistingue il QuickSort e gli fa valere questo nome è il fatto che il caso peggiore è raro, mentre nel caso tipico, supponendo che tutte le posizioni $p$ del pivot siano equiprobabili, il caso medio è la media dei tempi:
$$
T_{QuickSort} = T_{Partition} + \frac{1}{n} \sum_{p=1}^{n-1} (T_{QuickSort}(p) + T_{QuickSort}(n-p))
$$
Srotoliamo $\displaystyle\sum_{p=1}^{n-1} (T_{QuickSort}(p) + T_{QuickSort}(n-p))$:
$$
\begin{align}
\sum_{p=1}^{n-1} (T(p) + T(n-p)) & = T(1) + T(n-1) + T(2) + T(n-2) + \ldots + T(n-1) + T(1) \\
 & = T(1) + T(2) + \ldots + T(n-1) + T(1) + T(2) + \ldots + T(n-1) \\
 & = 2\sum_{p=1}^{n-1} T(p)
\end{align}
$$
Di qui, con un po' di passaggi algebrici, possiamo stabilire la disequazione:
$$
T(n) \le 2(n+1) \sum_{k=3}^{n+1} \frac{1}{k}
$$
Dato che cerchiamo un confine superiore per calcolare $O(T(n))$, possiamo ridurci a risolvere la relazione di ricorrenza ponendoci nell'estremo:
$$
T(n) = 2(n+1) \sum_{k=3}^{n+1} \frac{1}{k}
$$
Sappiamo che $2(n+1)$ è lineare, però qual è l'ordine di grandezza di $\displaystyle\sum_{k=3}^{n+1} \frac{1}{k}$? Si ricorre a un trucchetto: $k$, se da variabile discreta diventa continua, fa diventare quella sommatoria un integrale:
![](Pasted%20image%2020241230035938.png)
(in questo esempio $k$ è diventata $x$)
Abbiamo quindi:
$$
\sum_{k=3}^{n+1} \frac{1}{k} < \int_2^{n+1} \frac{1}{x} dx = \ln(n+1) - \ln 2 \in O(\log n)
$$
Attenzione: questo somiglia all'integrale di Riemann, ma NON È L'INTEGRALE DI RIEMANN

Abbiamo che $\displaystyle\sum_{k=3}^{n+1} \frac{1}{k} \in O(\log n)$, quindi $T(n) = O(n \log n)$.

![](Pasted%20image%2020241230040911.png)

# Fonti

- Videolezioni del Prof. De' Liguoro Ugo del corso di Algoritmi e Strutture Dati (canale B), Corso di Laurea in Informatica presso l'Università di Torino, A.A. 2024-25:
	- [Relazioni di ricorrenza (1) - 11/10/24](https://unito.webex.com/recordingservice/sites/unito/recording/9748cf6069f3103dadeeea775fad1fb2/playback)
	- [Relazioni di ricorrenza: Merge-Sort, Quick-Sort - 14/10/24](https://unito.webex.com/recordingservice/sites/unito/recording/d7b8a69e6c52103d8f7fc2cf82972f1a/playback)
