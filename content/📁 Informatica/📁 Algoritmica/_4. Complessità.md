---
draft: true
---
Confrontare due algoritmi che risolvono stesso problema = quale è più efficiente (complesso computazionalmente)?
Dipende da quale risorsa considero:
- Tempo (non tempo fisico perché dipende dall'architettura della macchina, ma relato a esso)
- Spazio = quantità di memoria necessaria per eseguire l'algoritmo
- Complessità circuitale o hardware (una volta teorico, ora è un problema corrente) = numero di processori, numero dei componenti (porte logiche) di un circuito, ecc.

La domanda "Quale algoritmo è più veloce?" ci interessa perché:
- Per capire quanto tempo ci vuole per eseguire un programma che lo implementa.
- Per stimare la grandezza massima dell'ingresso di un'esecuzione ragionevole
- Per confrontare l'efficienza di più algoritmi che risolvono lo stesso problema
- Per sapere quale parte del codice sarà eseguita più volte (lavoro di ottimizzazione del codice)

La complessità è matematicamente una funzione: in ingresso prende le risorse, in uscita ci dà la complessità. Quindi dipende dall'ingresso.

Funzione tempo. Possiamo calcolare:
- Il numero dei secondi (dipendente dalla macchina, come detto prima)
- Il numero delle operazioni elementari, ciascuna con un proprio coefficiente
- La maggior parte delle volte, il numero delle volte che una specifica operazione viene eseguita.

Esempio: minimo in un vettore
![](Pasted%20image%2020241221192931.png)

Dipende dalla dimensione dell'istanza.
$\le k - j$ perché dipende dall'if ed è uguale a $k-j$ solo quando $A[j,\ldots,k]$ è ordinato in senso decrescente.

![](Pasted%20image%2020241221193050.png)

Dalla combinazione lineare esce un polinomio di primo grado.

# Dimensione dell'ingresso

Nel caso di **Minimo(A,j,k)**, per quanto riguarda la dimensione dell'ingresso, ciò che conta è il numero degli elementi in $A[j,\ldots,k]$, non il loro valore.

In generale, la dimensione dell'ingresso è una misura della sua rappresentazione (a meno di una costante):

|m| = dimensione di m = numero di bit per rappresentare m = $\lfloor \log_2 (m+1) \rfloor + 1$

$|A[0,\ldots,n-1]$ = dimensione di $A[0,\ldots,n-1]$ = $n \cdot c$ dove c = numero bit del generico elemento di $A[]$.

Nel seguito useremo $c=1$ perché moltiplicare per una costante (come vedremo) non conta dal punto di vista dell'analisi asintotica.

# Ingresso di dimensione n

Supponiamo di voler esprimere T in funzione della dimensione dell'istanza, invece che dell'istanza stessa:
|x| = |y| NON implica che T(x) = T(y) (es. algoritmo di ordinamento, due array entrambi da 8 elementi ma il primo è già ordinato e il secondo no, il tempo del primo sarà minore di quello del secondo).

Distinguiamo allora i casi:
$$
T_{\text{migliore}}(n) = \min\{T(x) : |x| = n\}
$$
cioè, x è il caso migliore quando $T_{\text{migliore}}(|x|)=T(x)$

A noi però interessa:
$$
T_{\text{peggiore}}(n) = \max\{T(x) : |x| = n\}
$$
cioè, x è il caso migliore quando $T_{\text{peggiore}}(|x|)=T(x)$

Attenzione: quando si fa il massimo, c'è bisogno che l'insieme sia limitato. Ma siccome noi assumiamo che tutti i dati siano rappresentati in modo discreto con un vocabolario finito, allora c'è sempre un massimo per ogni n.

Per l'esempio di **Minimo()**, il caso migliore è quando il minimo è il primo elemento, il caso peggiore è quando l'array è ordinato in senso decrescente e il minimo è l'ultimo elemento.

# Come confrontare funzioni

Sappiamo che il tempo di calcolo non è un numero ma una funzione e, quindi, per confrontare il tempo di calcolo di due algoritmi dobbiamo confrontare tra loro funzioni.

Di queste funzioni possiamo conoscere il loro grafico, ma le loro curve sono composte da infiniti punti e non posso confrontare infiniti valori. Ciò che si fa quindi è vedere il loro comportamento agli asintoti, cioè ai loro estremi.

# Le costanti non contano

Non solo consideriamo solo il comportamento agli asintoti, ma c'è il discorso delle costanti che vengono eclissate.

Esempio:
D = dimensione massima in bit di un problema trattabile col computer C1 usando un algoritmo che impiega tempo T(n)=n^2, cioè esponenziale.
Costruiamo un computer C2 1000 volte più veloce C1: in pratica buona idea, ma in teoria no perché, se $\frac{2^{D'}}{1000}=2^D$, allora $D'$ (cioè dimensione max di C2) e uguale a:
$$
\frac{2^{D'}}{1000} = 2^D \iff 2^{D'} = 1000 \cdot 2^D \iff D' = \log_2 1000 + D \approx 10+D
$$
Cioè non cambia tantissimo rispetto a D, ho guadagnato solo 10 bit. Ciò insegna che quando le complessità sono così alte la differenza non la fa la macchina, ma l'algoritmo vero e proprio: le costanti non contano, ergo si può ometterle considerandole tutte pari a $1$.

# O-grande

O(g(n)) dove g() è una funzione asintoticamente positiva (ma tutte le funzioni, esprimendo il tempo, sono per forza positive, non ha senso avere costi di tempo negativi, non si può guadagnare il tempo).
g() è un confine superiore ad f(), ma non in tutti i punti di g() ed f() ma solo agli asintoti.
$$
f(n) \in O(g(n)) \iff \exists c > 0, n_o \forall n \ge n_0: f(n) \le cg(n)
$$
cioè: una funzione f(n) fa parte di una classe O di una funzione g() se e solo se esiste una costante c reale positiva e una costante n_0 naturale positivo tale che da quel punto in poi ($\forall n \ge n_0$) la nostra funzione f è limitata superiormente dalla funzione g() moltiplicata per la costante c.
Definizione di P. Bachman del 1892, fa parte delle notazioni delle serie, sviluppi di serie ecc.

## ?

Due casi strani:
1. Quantificazione a meno di un numero finito di casi
2. Astrazione da costanti moltiplicative

Si definisce quindi un confronto per punti quasi ovunque, a meno di un numero finito di casi.

$$
f \le g \iff \exists z \forall x \ge z: f(x) \le g(x)
$$
cioè: la funzione f è asintoticamente minore o uguale a g ($f \le g$) se e solo se le immagini di x sono minori o uguali ($f(x) \le g(x)$) per tutti i valori di $x$ maggiori o uguali a partire da un certo $z$ ($\exists z \forall x \ge z$).

Il suo duale è:
$$
\lnot(f \le g) \iff \forall z \exists x \ge z: \lnot(f(x) \le g(x))
$$
cioè: esistono infiniti x (non necessariamente continui) tali che NON vale questa relazione, cioè questa relazione non vale mai.

# Relazione di pre-ordine

La relazione $f \le g$ è soltanto un preordine (riflessiva e transitiva, ma non necessariamente antisimmetrica).

## Inconfrontabilità

Sugli interi l'ordine è totale (se ho due interi, so perfettamente qual è il loro ordine), ma ci sono alcune relazioni che non sono totali (es. con la relazione di inclusione, due insiemi singoletti con un ognuno un elemento diverso, questi due singoletti sono inconfrontabili perché uno non è mai incluso nell'altro, eppure la relazione di inclusione È una relazione d'ordine ma PARZIALE).

Ci sono funzioni inconfrontabili. Esempio:
$$
f(x) =
\begin{cases}
1 & \text{se $x$ pari} \\
0 & \text{se $x$ dispari}
\end{cases}
$$
e
$$
g(x) =
\begin{cases}
0 & \text{se $x$ pari} \\
1 & \text{se $x$ dispari}
\end{cases}
$$

## Non antisimmetriche

Prese due funzioni qualsiasi f e g, $f \le g$ non è un ordine parziale, non essendo antisimmetrica:
$$
f \le g \land g \le f \iff \exists z \forall x \ge z: f(x) = g(x)
$$
cioè: quando ho $f \le g \land g \le f$, se fosse una relazione di ordine, dovrei avere che f e g dovrebbero essere uguali SEMPRE, ma ciò non è vero perché sono uguali solo a partire da un certo z, quindi non sono la stessa funzione.

# Le costanti non contano

Teorema: Per ogni f,g e per ogni costante c > 0:
$$
f(n) \in O(cg(n)) \iff f(n) \in O(g(n))
$$
Questo significa che il comportamento asintotico di $f(n)$ rispetto a $g(n)$ è indipendente dal fattore costante $c > 0$. Vediamo la dimostrazione passo per passo.

Dimostrazione della direzione "$\Rightarrow$":
Se $f(n) \in O(cg(n))$, allora esistono una costante $d > 0$ e un valore di "soglia" $n_0$ tali che:
$$
f(n) \leq d \cdot (cg(n)), \quad \text{per ogni } n \geq n_0.
$$

Ora definiamo una nuova costante $b = d * c$, dove $c \neq 0$%%perché è =/=0?%%. Poiché $d > 0$ e $c > 0$, allora anche $b > 0$. Sostituendo $b$ nella disuguaglianza:

$$
f(n) \leq d \cdot cg(n) = b \cdot g(n).
$$

Questo implica $b = d \cdot c$ è la costante cercata e che $f(n) \in O(g(n))$, con la costante $b > 0$ e lo stesso $n_0$. Quindi, abbiamo dimostrato che se $f(n) \in O(cg(n))$, allora $f(n) \in O(g(n))$.

Dimostrazione della direzione "$\Leftarrow$":
Esistono una costante $d > 0$ e un valore di "soglia" $n_0$ tali  he $f(n) \le d \cdot g(n)$, per ogni $n \ge n_0$. Posto $b = \frac{d}{c}$ (ed esiste perché $c \ne 0$), abbiamo $b > 0$ e
$$
f(n) \le b \cdot cg(n)
$$
per ogni $n \ge n_0$.

Osservazione: per questo, $O(1) = O(k)$ per ogni $k$ costante è l'insieme delle funzioni superiormente limitate. Quindi $O(2)=O(57)=O(1.000.000)=O(1)$.
Tutte le funzioni costanti sono limitate, non tutte le funzioni limitate sono costanti.

# Sia le costanti che i gradi minori non contano

Teorema:
$$
3n^2 + 7n + 8 \in O(n^2)
$$

Dimostrazione:
Bisogna dimostrare che
$$
\exists c,n_0 \forall n \ge n_0: f(n) \le cg(n)
$$
dove $f = 3n^2 + 7 + 8$ e $g = n^2$, quindi trovare $c$ ed $n_0$ per cui vale la relazione.
Proviamo con $c=4$ e $n_0 = 1$:
$$
3 \cdot 1^2 + 7 \cdot 1 + 8 \not\le 4 \cdot 1^2
$$
Proviamo con $c = 4$ e $n_0 = 9$:
$$
3 \cdot 9^2 + 7 \cdot 9 + 8 = 314 \le 324 = 4 \cdot 9^2
$$
Funziona, quindi già questo basta, perché sono operazioni monotone: facendo crescere i valori la relazione si mantiene.
Proviamo a cercare la scelta migliore, cioè l'eguaglianza: si ottiene con $c = 4$ e $n_0 = 8$.

Serve fare questo tipo di discorsi? Non sempre. Qualche volta sì, ma in generale no.

Quindi posso dire che $n \ge 8 \implies 3n^2 + 7n + 8 \le 4n^2$.
La tesi equivale a $7n + 8 \le 4n^2 - 3n^2 = n^2$.
Dividendo per n: $7 + \frac{8}{n} \le n$.
Ma $n \ge 8$, allora $\frac{8}{n} \le \frac{8}{8}=1$ e, quindi, $7 + \frac{8}{n} \le 7 + 1 = 8 \le n$.

## Osservazione

Se $p(n)$ è una funzione polinomiale asintoticamente positiva associata a un polinomio di grado $k$, allora $p(n) \in O(n^k)$.

Dimostrazione:
$$
p(n) = \sum_{i=0}^k a_in^i
$$
Dichiaro $a = \max\{a_i \mid \forall i: 0 \le i \le k\}$, quindi
$$
p(n) = \sum_{i=0}^k a_in^i \le \sum_{i=0}^k an^i
$$
Porto $a$ davanti a fattor comune:
$$
p(n) = \sum_{i=0}^k a_in^i \le \sum_{i=0}^k an^i = a \cdot \sum_{i=0}^k n^i
$$
Monotonicità della funzione esponenziale ($n^i \le n^k$ se $i$ va da 0 a k). Quindi:
$$
p(n) = \sum_{i=0}^k a_in^i \le \sum_{i=0}^k an^i = a \cdot \sum_{i=0}^k n^i \le a(k+1)n^k
$$
Quel $a(k+1)n^k$ è una costante che cerchiamo.

Quindi: i termini di grado inferiore si possono ignorare.

# Inclusioni

## Inclusioni 1

Ma questa in realtà, è una caratteristica: il grado del polinomio caratterizza l'O-grande: cioè non solo $p(n) \in O(n^k)$, ma se $p(n)$ è un polinomio di grado $k$ e coefficienti positivi, allora $p(n) \notin O(n^j)$ con $j < k$.
Ovviamente però è vera l'inclusione: se $p(n) \in O(n^k)$, allora sarà anche $p(n) \in O(n^h)$ con $h \ge k$.

Dimostrazione per assurdo:
Se $p(n) \in O(n^j)$, allora se $a$ è il coefficiente direttore di $p$, quasi ovunque ho che:
$$
an^k \le p(n) \le cn^j
$$
per qualche $c > 0$. Ma
$$
an^k \le cn^j \iff n^{k-j} \le \frac{c}{a} \iff n \le \sqrt[k-j]{\frac{c}{a}} = d
$$
chiamiamo arbitrariamente quel numeraccio $d$. Contraddizione quando $n > \lceil d \rceil$.

Morale: tutto ciò che conta in un polinomio è il grado.

Riassumendo: se $h \ge k > j$, allora $O(n^h) \subseteq O(n^k)$ ma $O(n^j) \not\subsetneq O(n^k)$.

## Inclusioni di logaritmi

$$
O(\log_a n) = O(\log_b n)
$$
con $a,b > 1$.
Ciò lo sappiamo perché si può cambiare la base di un algoritmo:
$$
\log_a n = \frac{\log_b n}{\log_b a} = \frac{1}{\log_b a} \log_b n
$$
ma $\frac{1}{\log_b a}$ è una costante che si può ignorare.

Quindi la base nei logaritmi non importa e scriviamo semplicemente $O(\log n)$.

## $O(1) \subseteq O(\log n)$

Perché $O(1)$ è la classe delle funzioni limitate superiormente ($f(n) \in O(1) \implies \exists c \forall n_0 \ge n: f(n) \le c$), ma $\log n$ è superiormente illimitata.
Quindi le funzioni limitate a un certo momento saranno sotto le funzioni illimitate.

## $O(\log n) \subseteq O(n)$

Ragionando per "quasi ovunque a partire da un $k$":
$$
\log_k n \le n \iff n = k^{\log_k n} \le k^n
$$
perché $n \le k^n$ per ogni $n$ e per ogni $k \ge 1$%%perché non può essere <1?%%.
%%inclusione propria o impropria?%%

## $O(n) \subseteq O(n \log n)$

Ragionando per "quasi ovunque a partire da un $k$":
$$
n \ge k \implies \log_k n \ge \log_k k \implies \log_k n \ge 1 \implies n \log_k n > n
$$
%%inclusione propria o impropria?%%

## Esempio: $O(n^k) \subsetneq O(2^n)$

Usiamo questo esempio.
Classe $O(n^k)$: funzioni polinomiali. Unioni di tutte queste classi: POLI-TIME. Quando un algoritmo ha complessità polinomiale, allora quell'algoritmo è in $O(n^k)$ per qualche $k$.
Classe $O(k^n)$: funzione esponenziale, cioè parametro $n$ nell'esponente: EXP-TIME.

Dimostrare che POLI-TIME è incluso propriamente in EXP-TIME.
Facciamo ricorso alla teoria dei limiti, in particolare a:
$$
\lim_{n \to +\infty} \frac{f(n)}{g(n)} = 0
$$
con f,g funzioni asintoticamente positive e g mai nulla.

Abbiamo che:
$$
\lim_{n \to +\infty} \frac{f(n)}{g(n)} = 0 \implies f \in O(g) \land g \notin O(f)
$$
Abbiamo inclusione in un verso, ma non nell'altro.
Dimostriamo che $f(n) \in O(g(n))$:
per ogni $c > 0$ (con c che, tendendo allo 0, più piccolo è meglio è), abbiamo $0 \le \frac{f(n)}{g(n)} < c$ quasi ovunque, quindi $f(n) < cg(n)$ quasi ovunque.

quasi ovunque vuol dire:
$$
q.o. P(n) \iff \exists n_0 \forall n > n_o : P(n) \iff \forall^\infty n : P(n)
$$

Dimostriamo per assurdo che $g(n) \notin O(f(n))$:
$$
g(n) \in O(f(n)) \implies \exists c < 0: g(n) \le cf(n) q.o. \implies \frac{1}{c} \le \frac{f(n)}{g(n)} q.o.
$$
cioè, il mio rapporto $\frac{f(n)}{g(n)}$ sarà quasi ovunque $\ge \frac{1}{c}$, quindi non potrà mai essere compreso tra 0 e c, quindi contraddizione con il limite.
Dalla contraddizione segue la tesi.



In particolare, sappiamo che $\displaystyle \lim_{n \to +\infty} \frac{n^k}{2^n} = 0$, quindi questa inclusione sussiste ma è propria perché trivialmente $O(n^k) \subsetneq O(2^n)$ ma non $O(n^k) = O(2^n)$.
Ciò mi insegna che le funzioni esponenziali crescono più velocemente delle polinomiali.

# o-piccolo

Se $f(n) \in O(g(n))$ e $g \le h$, allora $f(n) \in O(h(n))$: quanto "buono" è questo confine superiore?

Definizione (o-piccolo di una funzione anche detto "insieme degli infinitesimi di quella funzione"):
$$
f(n) \in o(g(n)) \iff \forall c > 0, \forall^\infty n: 0 \le f(n) < cg(n)
$$
(ovviamente sempre funzioni asintoticamente positive ecc. ecc.)
Da confrontare con
$$
f(n) \in O(g(n)) \iff \exists c > 0, \forall^\infty n: 0 \le f(n) < cg(n)
$$
Sola differenza: in O, chiediamo che ESISTA, mentre in o vogliamo PER OGNI.

Se $f(n) \in o(g(n))$, allora $f$ è un INFINITESIMO di g, quindi g non è un confine superiore "stretto" di f.

Osserviamo infatti che, se g è positiva non nulla:
$$
f(n) \in o(g(n)) \implies \lim_{n \to +\infty} \frac{f(n)}{g(n)} = 0
$$

Osserviamo inoltre che $o \subsetneq O$ perché il PER OGNI implica l'ESISTE, ma non è vero che l'ESISTE implica il PER OGNI.

# Funzioni ordinate per velocità di crescita

è una scala non discreta, ci sono infinite altre funzioni nel mezzo.

Vale la proprietà transitiva per le inclusioni.

Oltre ci sono le funzioni iperesponenziali: parametro sia alla base che all'esponente es. n^n, n^n^n...^n ecc.

Poli-logaritmica = logaritmica ma con esponente costante

Poli-esponenziali = esponente è un polinomio

# Algebra di O-grande

Spesso leggiamo eguaglianze del tipo $\frac{1}{2}n^2 + n = O(n^2)$ oppure $\frac{1}{4}n^2 = O(n^2)$, cioè = anziché $\in$.
In questo modo, sto buttando via i dettagli per ricordare solo la complessità. C'è perdita di informazione nel passare da sinistra a destra dell'=.
È un uso spropositato, perché se le prendo alla lettera, ottengo assurdità del tipo $\frac{1}{2}n^2 + n = \frac{1}{4}+n^2$.
Proprio per questo, $f(n) = O(g(n))$ si interpreta come un'equazione "a senso unico", con cui rimpiazziamo una funzione con il suo ordine di grandezza.

Definiamo quindi l'algebra di O-grande in modo da poter "connettere" semplici funzioni con classi di funzioni di O-grande:
$$
f(n) + O(g(n)) = \{ h \mid \exists g' \in O(g(n)), \forall^\infty n: h(n) \le f(n) + g'(n) \}
$$
e
$$
f(n) \cdot O(g(n)) = \{ h \mid \exists g' \in O(g(n)), \forall^\infty n: h(n) \le f(n) \cdot g'(n) \}
$$
La relazione $f(n) + O(g(n))$ esprime che una funzione $h(n)$ è dominata da una somma di $f(n)$ e da una funzione che cresce come $g(n)$. La funzione risultante non cresce più velocemente di $f(n)$ più una funzione che è al massimo un multiplo di $g(n)$.

Ne segue quindi che:
$$
f(n) + O(g(n)) = O(f(n) + g(n))
$$
e
$$
f(n) \cdot O(g(n)) = O(f(n) \cdot g(n))
$$

Similmente definiamo (tramite estensione di Frobenius dell'operazione che avevo fatto prima):
$$
O(f(n)) + O(g(n)) = \{ h \mid \exists f' \in O(f(n)), \exists g' \in O(g(n)), \forall^\infty n: h(n) \le f'(n) + g'(n) \}
$$
e, analogamente, $O(f(n)) \cdot O(g(n))$.

In questo modo posso fare operazioni tra funzioni e classi, ma anche classi tra loro.

## Derivazione

1:
$$
f(n) = O(f(n))
$$

2:
$$
cO(f(n)) = O(f(n))
$$
perché $cO(f(n)) = O(cf(n)) = O(f(n))$ perché le costante sono da ignorare.
Ciò significa che, se abbiamo operazioni di complessità $O(f(n))$ che vengono eseguite $c$ volte, a noi interessa solo il fatto che sono di complessità $O(f(n))$, non il numero di ripetizioni.

3:
$$
O(f(n)) + O(g(n)) = O(f(n) + g(n))
$$

4 (proprietà di assorbimento della somma tra classi):
$$
O(f(n)) + O(f(n)) = O(f(n))
$$
perché $O(f(n)) + O(f(n)) = O(f(n) + f(n)) = O(2f(n)) = O(f(n))$ perché costanti da ignorare.

5:
$$
f \le g \implies O(f(n)) + O(g(n)) = O(g(n))
$$
La somma diventa l'operatore massimo.

6:
$$
O(f(n)) \cdot O(g(n)) = O(f(n) \cdot g(n))
$$

## Errore da evitare:

$$
O(1) + O(1) = O(2) = O(1)
$$
Allora allo stesso modo dovrebbe essere
$$
\underbrace{O(1) + O(1) + \ldots + O(1)}_{\text{$n$ volte}} = n \cdot O(1)
$$
dovrebbe essere uguale a $O(1)$? SBAGLIATO, è uguale a $O(n)$ perché $n$ è una variabile, un parametro, non una costante!

es. in un for ho un'operazione di costo $O(1)$, ma fa differenza se la faccio 1, 2 o 500000 volte.

### Esempietto:

$$
\begin{align*} % Algorithm environment
 & \textbf{MatriceIdentità(Matrice A, int n)} \quad \rhd \\ & \text{for } i = 1 \text{ to } n \text{ do} \quad \rhd T_3 \\
 & \quad \text{for } j = 1 \text{ to } n \text{ do} \quad \rhd T_2 \\
 & \quad \quad A[i][j] = 0 \quad \rhd T_1 \\
 & \quad \text{end for} \\
 & \text{end for} \\
 & \text{for } i = 1 \text{ to } n \text{ do} \quad \rhd T_5 \\
 & \quad \quad A[i][i] = 1 \quad \rhd T_4 \\
 & \text{end for} \\
 & \text{return }
\end{align*}
$$
%% 
Formattare meglio usando right brackets come qua:
$$
\left.
\begin{array}{ll}
2x=X-Y-Z+W & 2y=-X+Y-Z+W \\
2z=-X-Y+Z+W & 2w=X+Y+Z+W
\end{array}
\right\} \rhd prova
$$
%%
Abbiamo:
- $T_1(n) \le c_1 = O(1)$;
- $T_4(n) \le c_2 = O(1)$;
- $T_2(n) = n \cdot T_1(n) = n \cdot O(1) = O(n)$;
- $T_5(n) = n \cdot T_4(n) = n \cdot O(1) = O(n)$;
- $T_3(n) = n \cdot T_2(n) = n \cdot O(n) = O(n^2)$;
- quindi $T(n) = T_3(n) + T_5(n) = O(n^2) + O(n) = O(n^2)$.

Se blocchi uno dentro l'altro -> moltiplicazione
Se blocchi in sequenza -> addizione

# Omega

Uno dice $\sqrt n \in O(2^n)$: è vera, ma banale, non è informativa. Quasi tutti gli algoritmi trattati sono esponenziali.
L'altro dice: naturalmente si tratta di un'asserzione vera, ma il confine $2^n$ non è stretto.

Per definire "stretto", introduciamo la notazione Omega:
$$
f(n) \in \Omega(g(n)) \iff \exists c > 0, n_0 \forall n \ge n_0 : cg(n) \le f(n)
$$
Duale di O-grande: questo fissa un confine inferiore.

# Classi Theta

$$
f(n) \in \Theta(g(n)) \iff c_1,c_2 > 0, n_0 \forall n \ge n_0: c_1g(n) \le f(n) \le c_2g(n)
$$

Theta è l'unione di O-grande e Omega: le due funzioni $c_1g$ e $c_2g$ formano una sorta di "fascia" entro la quale varia la f e dalla quale non può scappare. Questo ricorda un po' il Teorema dei Due Carabinieri in analisi.
Attenzione: g è sempre la stessa, sono solo le costanti $c_1$ e $c_2$ che creano la "fascia" entro cui si può muovere f.

# Relazioni tra le notazioni

1:
$$
f(n) \in O(g(n)) \iff g(n) \in \Omega(f(n))
$$
perché $f(n) \le c g(n) \iff \frac{1}{c}f(n) \le g(n)$

2:
$$
f(n) \in \Theta(g(n)) \iff f(n) \in \Omega(g(n)) \land f(n) \in O(g(n))
$$

# Relazioni tra $\Theta$ e i limiti

Teorema:
$$
\lim_{n \to +\infty} \frac{f(n)}{g(n)} = a \land 0 < a < +\infty \implies f(n) \in \Theta(g(n))
$$
Dimostrazione (per ipotesi sulla definizione di limite, con $\delta = n_0$):
$$
\forall \epsilon > 0, \exists n_0 \forall n \ge n_0 \left| \frac{f(n)}{g(n)} - a \right| < \epsilon
$$
Se ciò vale per ogni epsilon, pongo $\epsilon = \frac{a}{2} > 0$. Problema: non so qual è la relazione, quindi vedo tutti i casi. Primo caso:
$$
\begin{align}
\frac{f(n)}{g(n)} \ge a & \implies \left| \frac{f(n)}{g(n)} - a \right| = \frac{f(n)}{g(n)} - a < \frac{a}{2} \\
 & \implies \frac{f(n)}{g(n)} < a + \frac{a}{2} = \frac{3}{2} a \\
 & \implies f(n) < \frac{3}{2} ag(n)
\end{align}
$$
Tolgo il valore assoluto perché la differenza sarà sempre positiva dato che $\frac{f(n)}{g(n)} \ge a$, quindi porto $a$ dall'altra parte e risolvo.
Esce che $\frac{3}{2} a$ è una costante, quindi apposto.
Secondo caso:
$$
\frac{f(n)}{g(n)} < a \implies f(n) < ag(n) \implies f(n) < ag(n) < \frac{3}{2}ag(n)
$$
Anche in questo caso risulta che $f(n) < \frac{3}{2} ag(n)$.
In conclusione abbiamo confermato che $f(n) \in O(g(n))$.

Abbiamo verificato l'O-grande, ora manca l'omega (dimostrare che $f(n) \in \Omega(g(n))$). Per farlo, sfruttiamo una proprietà dei limiti.
$$
\begin{align}
\lim_{n \to +\infty} \frac{f(n)}{g(n)} = a & \implies \lim_{n \to +\infty} \frac{g(n)}{f(n)} = \frac{1}{a} > 0 \\
 & \implies g(n) \in O(f(n)) \\
 & \implies f(n) \in \Omega(g(n))
\end{align}
$$
Il passaggio nella prima riga è dato dal fatto che il limite di un rapporto è uguale al rapporto dei limiti, perché la divisione (che è in realtà l'altra faccia della moltiplicazione) è una funzione continua, la composizione di funzioni continue è continua e le funzioni continue passano i limiti.

Avendo verificato sia O-grande che Omega, abbiamo Theta.

# Fonti

- Videolezioni del Prof. De' Liguoro Ugo del corso di Algoritmi e Strutture Dati (canale B), Corso di Laurea in Informatica presso l'Università di Torino, A.A. 2024-25:
	- [Il tempo di calcolo (1) - 4/10/24](https://unito.webex.com/recordingservice/sites/unito/recording/69c1256b647b103da6160e9add92f4ee/playback)
	- [Il tempo di calcolo (2) - 7/10/24](https://unito.webex.com/recordingservice/sites/unito/recording/9305547966d3103db7700e62f8a0aa4a/playback)
	- [Il tempo di calcolo (3) - 11/10/24](https://unito.webex.com/recordingservice/sites/unito/recording/82eefd7769ef103dbeff4694df965fcd/playback)
