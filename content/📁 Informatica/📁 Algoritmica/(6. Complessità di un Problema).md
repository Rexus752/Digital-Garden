---
draft: true
---
Esistono degli algoritmi ancora migliori del QuickSort e fino a che livello posso migliorare la complessità di un algoritmo? Teoricamente, dovrebbe esistere un confine INFERIORE oltre il quale non si può andare con la complessità, cioè una complessità minima.
Come si può dimostrare l'esistenza di un confine INFERIORE per ogni algoritmo? E come si può raggiungere?

# Esempio: elementi "gemelli" in un array

Dato un array $A[1,\ldots,n]$ di interi, decidere se esistono due indici $i \ne j$ tali che $A[i] = A[j]$. Come si può risolvere efficientemente?
$$
\begin{align*} % Algorithm environment
 & \textbf{Twins}(A[1,\ldots,n]) \\
 & \rhd \textrm{Pre: }  \\
 & \rhd \textrm{Post: }  \\
 & \textrm{for } i \gets 1 \textrm{ to } n \textrm{ do} \\
 & \quad \textrm{for } j \gets 1 \textrm{ to } n \textrm{ do} \\
 & \quad \quad \textrm{if } i \ne j \textrm{ and } A[i] = A[j]  \textrm{ then} \\
 & \quad \quad \quad \textrm{return } true \\
 & \textrm{return } false
\end{align*}
$$

Questo algoritmo richiede tempo $O(n^2)$.
Si può migliorare considerando il fatto che la stessa coppia $i$ e $j$ viene considerata due volte, cioè il ciclo considera $i=j=1$, $i=j=2$ e così via fino a $i=j=n$. Migliorato:
$$
\begin{align*} % Algorithm environment
 & \textbf{Twins'}(A[1,\ldots,n]) \\
 & \rhd \textrm{Pre: }  \\
 & \rhd \textrm{Post: }  \\
 & \textrm{for } i \gets 1 \textrm{ to } n - 1 \textrm{ do} \\
 & \quad \textrm{for } j \gets i + 1 \textrm{ to } n \textrm{ do} \\
 & \quad \quad \textrm{if } A[i] = A[j]  \textrm{ then} \\
 & \quad \quad \quad \textrm{return } true \\
 & \textrm{return } false
\end{align*}
$$
Twins' è migliore di Twins, ma asintoticamente no, perché
$$
T_{Twins'}(n) = \sum_{i=1}^n i = \frac{n(n+1)}{2}
$$
che è quadratico allo stesso modo.

Si può ulteriormente migliorare se si considera un array ordinato:
$$
\begin{align*} % Algorithm environment
 & \textbf{Twins''}(A[1,\ldots,n]) \\
 & \rhd \textrm{Pre: }  \\
 & \rhd \textrm{Post: }  \\
 & Merge-Sort(A[1,\ldots,n]) \\
 & \rhd \textrm{In un array ordinato, eventuali "gemelli" sono adiacenti} \\
 & \textrm{for } i \gets 1 \textrm{ to } n - 1 \textrm{ do} \\
 & \quad \textrm{if } A[i] = A[i+1] \textrm{ then} \\
 & \quad \quad \textrm{return } true \\
 & \textrm{return } false
\end{align*}
$$
Quindi prima ordina col Merge-Sort, poi controlla gli uguali. Il confronto diventa di costo lineare, mentre l'ordinamento è $n \log n$:
$$
T_{Twins''}(n) = O(n \log n) + O(n) = O(n \log n)
$$
che è migliore del quadratico.

Ma a questo punto, qual è un tempo di calcolo SUFFICIENTE alla risoluzione del problema dei Gemelli (cioè un confine superiore)?

Confine SUPERIORE alla complessità di un problema: un confine superiore per il tempo di calcolo (nel caso peggiore) di UN algoritmo che risolve il problema

Per il SUFFICIENTE, basta che si trovi un algoritmo che mi risolva il problema (in questo caso n log n).

E il tempo di calcolo NECESSARIO alla risoluzione del problema?

Confine INFERIORE alla complessità di un problema: un confine inferiore per il tempo di calcolo (nel caso peggiore) di TUTTI gli algoritmi che risolvono il problema.

Vogliamo una f che sia l'Omega di tutte le infinite possibili soluzioni di un problema algoritmo (qualora esso sia solubile, perché o è insolubile come il problema dell'halt o ha infinite soluzioni), cioè una f per la quale non si possa fare meglio di così.

# Confini banali

## Dimensione dei dati

Quando è necessario esaminare tutti i dati in ingresso, ovvero generare tutti i dati in uscita.
Es: la moltiplicazione di due matrici quadrate di ordine $n$ richiede l'ispezione di $2n^2 \in \Omega(n^2)$ entrate. Questo perché non posso avere la moltiplicazione di due matrici senza prendere in considerazione TUTTI i dati nelle matrici.

## Eventi contabili

Quando c'è un evento la cui ripetizione un numero contabile di volte sia necessaria alla soluzione del problema.
Es: la determinazione del massimo tra $n$ elementi richiede almeno $n-1 \in \Omega(n)$ confronti, in cui altrettanti elementi non massimi risultino minori. L'evento contabile è il confronto, in questo caso.

Nel caso del nostro algoritmo dei gemelli (che abbiamo migliorato fino a n log n), sappiamo che il confine inferiore è n, quindi tra n log n ed n c'è quello che viene detto "gap algoritmico":
![](Pasted%20image%2020241230164830.png)

# Esempio: somma17

Dato un array di $n$ interi positivi, decidere se ne contiene due la cui somma sia $17$ (17 è scelto solo perché serve un numero dispari per questo esercizio).

Soluzione banale:
$$
\begin{align*} % Algorithm environment
 & \textbf{Sum17}(A[1,\ldots,n]) \\
 & \rhd \textrm{Pre: } \forall i \in \{1,\ldots,n\}, A[i] \ge 0 \\
 & \rhd \textrm{Post: }  \\
 & \textrm{for } i \gets 1 \textrm{ to } n - 1 \textrm{ do} \\
 & \quad \textrm{for } j \gets i + 1 \textrm{ to } n \textrm{ do} \\
 & \quad \quad \textrm{if } A[i] + A[j] = 17  \textrm{ then} \\
 & \quad \quad \quad \textrm{return } true \\
 & \textrm{return } false
\end{align*}
$$
Molto simile a Twins', quindi il confine superiore è chiaramente quadratico. Il confine inferiore però è lineare per gli eventi confrontabili: infatti, contando il numero $n$ degli elementi dell'array e poiché ognuno di essi deve essere considerato almeno una volta, un confine inferiore alla complessità del problema Somma17 è $\Omega(n)$.

Come si raggiunge quel confine inferiore? Idea: calcoliamo il "vettore caratteristico" dell'insieme:
$$
C = \{ i \mid i \le 17 \land \forall j < n, i = A[j] \}
$$
cioè costruire un vettore di 18 booleani (da 0 a 17 compresi) per cui possiamo sapere se c'è una copia di quel numero all'interno del nostro array. Dato che $A[1,\ldots,n]$ ha solo interi positivi, la risposta sarà true se e solo se esistono $i,j \in C$ tali che $i + j = 17$.

![](Pasted%20image%2020241230170711.png)
Con questo vettore, ci metto un indice i all'inizio e un j alla fine che verranno incrementti e decrementati insieme in modo che la somma dei valori a cui puntano sia sempre 17. Ciò che dobbiamo fare è solo vedere se esistono contemporaneamente sia quell'i che quel j che ci danno come somma 17:
L'invariante di ciclo è "$i+j=17$", quindi l'output sarà true se e solo se $C[i]\land C[j] = true$.
L'algoritmo è:
$$
\begin{align*} % Algorithm environment
 & \textbf{Sum17'}(A[1,\ldots,n]) \\
 & \rhd \textrm{Pre: } \forall i \in \{ 1,\ldots,n \}, A[i] \ge 0 \\
 & \rhd \textrm{Post: }  \\
 & C[0,\ldots,17] \gets \textrm{un array di booleani} \\
 & \textrm{for } i \gets 0 \textrm{ to } 17 \textrm{ do} \\
 & \quad C[i] \gets false \\
 & \textrm{for } i \gets 1 \textrm{ to } n \textrm{ do} \\
 & \quad \textrm{if } A[i] \le 17 \textrm{ then} \\
 & \quad \quad C[A[i]] \gets true \\
 & i \gets 0 \\
 & j \gets 17 \\
 & \textrm{while } i < j \textrm{ and not } C[i] \land C[j] \textrm{ do} \\
 & \quad i \gets i + 1 \\
 & \quad j \gets j - 1 \\
 & \textrm{return } i < j
\end{align*}
$$
Il solo ciclo il cui tempo dipenda da $n$ è il secondo; il primo e il terzo richiedono tempo $O(18) = O(1)$! Quindi questo algoritmo è OTTIMO, perché questo algoritmo è della stessa classe del limite inferiore e meglio di così non si può fare.

In generale, definizione: un algoritmo $A$ è una SOLUZIONE ASINTOTICAMENTE OTTIMA di un problema $P$ se esiste $f(n)$ tale che:
1. Il tempo $T_A(n) \in O(f(n))$.
2. Un confine inferiore a $P$ è $\Omega(f(n))$.

# Il problema dell'ordinamento

Un confine superiore al problema è n log n (come il Merge-Sort).
Un confine inferiore basato sulla dimensione dei dati è $\Omega(n)$.
Esiste un confine inferiore migliore, che restringa il gap algoritmico tra n ed n log n?

L'idea alla base è quello di rappresentare tutti i possibili algoritmi astraendoci da ogni particolare struttura (altrimenti veniamo "vincolati" da essa) e la rappresentiamo in forma di albero. Un albero rappresenta un algoritmo:
- I nodi interni rappresentano le decisioni,
- Le foglie rappresentano le possibili uscite
- I rami rappresentano particolari esecuzioni.

L'albero di decisione che minimizza l'altezza fornisce un confine inferiore al numero di decisioni necessarie nel caso peggiore. 
Definizione: un albero binario è COMPLETO se ha $2^k$ vertici per ogni livello $k$ non vuoto. Un albero binario è QUASI PERFETTAMENTE BILANCIATO (qpb) o SEMI-COMPLETO se, avendo altezza $h$, è completo sino al livello $h-1$ con numero di foglie $s$:
$$
2^{h-1} \le s \le 2^h
$$
ovvero $h \ge \lceil \log_2 s \rceil$.
Quindi un problema la cui soluzione può rappresentarsi con un albero QPB con $s(n)$ possibili soluzioni è $\Omega(\log_2 s(n))$.

Se devo ordinare $n$ elementi, i possibili ordinamenti sono tante quante le permutazioni di $n$, cioè $n!$. Esempio di ordinamento di 3 elementi:
![](Pasted%20image%2020241230173029.png)

Ogni ramo rappresenta il minimo possibile dei confronti che devo fare per ordinare i 3 elementi, quindi l'altezza dell'albero è un confine inferiore al numero dei confronti che devo fare per ordinare 3 elementi. Questo albero è alto 3, infatti $h = \lceil \log_2 3! \rceil = 3$.

Nel caso dell'ordinamento:
- $s(n) = n!$ perché un ordinamento è una permutazione
- i nodi interni rappresentano i confronti

Ne concludiamo che un confine inferiore al problema dell'ordinamento è:
$$
\log_2 n! = \Omega(n \log n)
$$
cioè, il numero dei confronti deve essere dunque maggiore di questo. Possiamo evincere da ciò che il Merge-Sort è asintoticamente ottimo, ma non il Quick-Sort perché è quadratico nel caso peggiore.

# Un falso controesempio

Supponiamo di sapere che tutti i valori in $A[1,\ldots,n]$ siano compresi tra $0$ e $k$ . Si può ordinare $A$ in tempo migliore di $n \log n$?
Esiste una soluzione $O(n)$ molto simile a quella di Somma17.

Counting-Sort: l'idea è quella di contare quante volte un intero occorra in $A[1,\ldots,n]$ in tempo $O(n)$ usando un array $C[0,\ldots,k]$ di interi e poi, di copiare in $A[1,\ldots,n]$, $m = C[j]$ copie di $j$ per ogni $j \in \{ 0,\ldots,k \}$.

Quindi non sto più considerando il vettore caratteristico di un insieme, ma quello di un MULTIINSIEME (cioè collezioni in cui un elemento può avere una molteplicità).

$$
\begin{align*} % Algorithm environment
 & \textbf{Counting-Sort}(A[1,\ldots,n],k) \\
 & \rhd \textrm{Pre: } \forall p \in A[1,\ldots,n], o \le p \le k \\
 & \rhd \textrm{Post: }  \\
 & \textrm{for } i \gets 0 \textrm{ to } k \textrm{ do} \\
 & \quad C[i] \gets 0 \\
 & \textrm{for } i \gets 0 \textrm{ to } n \textrm{ do} \\
 & \quad C[A[i]] \gets C[A[i]] + 1 \\
 & \quad \rhd \forall j \in \{ 0,\ldots,k \}, C[j] = \textrm{il numero di volte che j occorre in } A[1,\ldots,n] \\
 & j \gets 0 \\
 & \textrm{for } i \gets 1 \textrm{ to } n \textrm{ do} \\
 & \quad \textrm{if } C[j] > 0 \textrm{ then} \\
 & \quad \quad A[i] \gets j \\
 & \quad \quad C[j] \gets C[j] - 1 \\
 & \quad \textrm{else} \\
 & \quad \quad j \gets j + 1
\end{align*}
$$

L'algoritmo potrebbe sembrare lineare: ma ciò va contro il calcolo fatto prima per cui gli algoritmi di ordinamento sono almeno n log n. Questo algoritmo, in realtà, ha complessità che dipende sia da n, che da k:
$$
T_{CountingSort}(n,k) = O(n) + O(k) = O(n + k)
$$
$k$ è calcolabile in tempo $O(n)$, ma tra $n$ e $k$ non vi è nessuna relazione, quindi %% ? %%

# Fonti

- Videolezioni del Prof. De' Liguoro Ugo del corso di Algoritmi e Strutture Dati (canale B), Corso di Laurea in Informatica presso l'Università di Torino, A.A. 2024-25:
	- [Confine inferiore ed ottimalità - 18/10/24](https://unito.webex.com/recordingservice/sites/unito/recording/341e2e8f6f72103dbef78e42bf240fbe/playback)